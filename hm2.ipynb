{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05581fb-18b9-41bf-b67e-e66d4962dc3b",
   "metadata": {},
   "source": [
    "1. У меня победила встроенная токенизация, у нее совсем немного выше и пресижен, и реколл, и следовательно общая точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2daddd2-fcdb-40aa-8bf7-6d5ec7e6b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from razdel import tokenize\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "data = pd.read_csv('labeled.csv')\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc302a25-3781-478c-ac5c-d12067e4fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Векторайзер с встроенным токенизатором\n",
    "vectorizer1 = TfidfVectorizer(min_df=10, max_df=0.3)\n",
    "X1 = vectorizer1.fit_transform(train.comment)\n",
    "X_test1 = vectorizer1.transform(test.comment) \n",
    "\n",
    "y1 = train.toxic.values\n",
    "y_test1 = test.toxic.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f36ee2-84c7-4861-97df-7ccbcdeb56b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 3364), (1442, 3364))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d8b249-1aeb-4667-8a88-e3160b8e4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Векторайзер с токенизатором razdel\n",
    "def razdel_tokenizer(text):\n",
    "    return [token.text for token in tokenize(text)]\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=razdel_tokenizer, min_df=10, max_df=0.3)\n",
    "X2 = vectorizer2.fit_transform(train.comment)\n",
    "X_test2 = vectorizer2.transform(test.comment) \n",
    "\n",
    "y2 = train.toxic.values\n",
    "y_test2 = test.toxic.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4d497b-c077-4f7a-b42c-c9b536c0be5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 3435), (1442, 3435))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape, X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a109f709-c3ee-4806-8c18-c39cc659c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.82      0.85       985\n",
      "         1.0       0.66      0.76      0.71       457\n",
      "\n",
      "    accuracy                           0.80      1442\n",
      "   macro avg       0.77      0.79      0.78      1442\n",
      "weighted avg       0.81      0.80      0.80      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Обучение с встроенной токенизацией\n",
    "clf1 = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf1.fit(X1, y1)\n",
    "preds = clf1.predict(X_test1)\n",
    "print(classification_report(y_test1, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b38a4b-0811-4ab2-9db1-ab00427e9622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.79      0.83       985\n",
      "         1.0       0.63      0.77      0.69       457\n",
      "\n",
      "    accuracy                           0.79      1442\n",
      "   macro avg       0.76      0.78      0.76      1442\n",
      "weighted avg       0.80      0.79      0.79      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Обучение с токенизацией razdel\n",
    "clf2 = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf2.fit(X2, y2)\n",
    "preds = clf2.predict(X_test2)\n",
    "print(classification_report(y_test2, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb2f36-aca2-446c-9938-4058beb7fb81",
   "metadata": {},
   "source": [
    "2. БОльшую разницу я ощутила не столько между классификаторами, сколько между векторайзерами, с каунтвекторайзером более топовыми получаются длинные тексты, с тфидф более короткие, я так испугалась такой сильной разницы, что сделала не два, а три классификатора, чтобы убедиться, что это точно из-за векторайзеров, а не потому что я с обучением налажала))) Токсик по сути есть почти во всех выведенных предложениях, но есть сомнения насчет 10го из второго и третьего классификаторов.\n",
    "   Больше совпадений получилось между вторым классификатором и третьим, но на самом деле это зависит от подборки текстов, иногда рандомфорест совсем что-то на своей волне выдает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f6f8cd-bc03-4fc1-9e8d-fe90f8a6235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sofya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.75      0.83       959\n",
      "         1.0       0.64      0.86      0.73       483\n",
      "\n",
      "    accuracy                           0.79      1442\n",
      "   macro avg       0.78      0.81      0.78      1442\n",
      "weighted avg       0.82      0.79      0.80      1442\n",
      "\n",
      "1. В Киеве на вокзале Мен було рок в 19, коли мене перший раз мав в зад хлопець рок в п д 30. Я тод перш рази став заходити на вокзал Ки в-Пасажирський в туалети - де були каб ни з д рками написи на ст нах. Так як досв ду ще не було н якого, то як знайомиться не уявляв. Сам перший природно не п дходив. А видивлявся на написи. дрочив св й член стоячи в каб нц . Хлопець був у сус дн й каб нц , в н побачив це, хитнув мен головою, запрошуючи п ти з ним. А так як н кого б льше в той момент не було, а був уже веч р, над на щось нше не було -все ж п шов за ним. У мене вже тод з явилася молофья - я вже спускав. Так як трохи ран ше ще не було, при дрочц робив це до при много стану - коли просто ставало дуже добре - але з хуя н чого не вид лялося. А до цього мен вже к лька раз в смоктали член хлопц мужики, я спускав м в рот, знав як це при мно. Ми прийшли б ля вокзалу кудись в кущ . В н розстебнув мен мотню, д став м й член став дрочити. А в той час нав ть це - коли хтось чужий рукою просто всього лише дрочив мен - було все одно дуже при мно. забирало. Бо коли тоб дрочать чужою рукою в дитинств - це вже щось: в д цього балд ш дуже. В н, ймов рно, здогадуючись, що перед ним зовс м новачок не намагався нав ть мен св й дати в руку: Так в н мене зав в , а пот м попросив повернутися: Я запитав нав що, справд не розум ючи нав що - а в н сказав треба так. я як теля повернувся п дкоряючись команд дорослого. В н приспустив мен штани, труси приставив до дерева у якого ми стояли, трохи нагнувши мене. А сам встав ззаду. По звуках я зрозум в, що в н розст ба соб свою мотню д ста св й член. В н притулився до мо поп сво м хуем, в д чого я здригнувся, але в н взяв мене за м й член знову став дрочити. А ншою рукою водити по стегнах з внутр шньо сторони. П д ймаючись в д кол н до поп - це посилювало кайф в д дрочк , я мл в, в н це теж в дчував, вже спок йно став тертися сво м хуем мен по поп . Пот м в н перестав дрочити мен , я почув як в н послинив св й член мою д рочку приставив мен св й член, в дсунув мене в д дерева трохи, пригнув мене почав засовували член в мене. Я стояв нагнувшись, упершись руками в дерево, н живий, н мертвий - перший раз в житт хлопець в мене засовував св й хуй! Я боявся - як все буде, що буде з мною, як це. Мен пощастило, звичайно, для першого разу, що у нього був маленький тонкий хуй. Тому н яких проблем у нього з всуванням його хуя в мою св жу попку не було. Оск льки мен не було боляче або непри мно я стояв не с паючись. Чекаючи як що буде дал :. В н засунув св й член весь в мене. т льки коли в н встромив його до к нця - було в дчуття що в н у щось уперся. Але не боляче зовс м. треба сказати чесно, що було при мно в дчувати, коли яйця його доторкнулися до мо попки, до д рочки, коли весь член був уже всередин не .. Це при мне в дчуття, коли умоглядно уявля ш що в тебе чийсь член засунуть: Це було мабуть нав ть при мн ше н ж все нше - в дчувати його яйця б ля очка. Коли весь член вже там. коли в н пот м став й бати мене, я намагався щоб част ше яйця його впиралися в попу мен , нав ть нод насаджувався сам глибше на його член, до упору. Але показувати що мен щось при мно тод здавалося ще не зручним - б льший час я просто стояв обхопивши дерево руками, а в н вставляв член в мене. Хоча особливого кайфу я ще тод не в дчував - було в дчуття - що просто в мене встромляв хлопець св й член ходив там. Так в н мав мене, продовжуючи одночасно весь цей час одн ю рукою дрочить мен - п дтримуючи в мен бажання: - ось в д цього мен було при мно. Природно. Це був його розрахунок. Я досить швидко в д дрочк чужою рукою спустив, в дразу з скочив з його члена. Але в дчув що у мене щось липкою ззаду на стегнах: Що щось тече по стегнах з очка. ось це мене засмутило сильно. вбило - я здогадався зрозум в що в н спустив в мене. Запитав, - Ти що ск нчив у мене? в н сказав - так. запитав мене - а ти що перший раз це робив? я мало не плачучи в д образи сказав - що так, перший раз: поставив йому дурне питання - нав що ти в мене спустив? Я не припускав цього, думав що в н просто посует ться в мене св й хуй все, а тут мен стало не по соб : було огидно, - особливо п сля того як сам пустив, - що на мен чужа гидота , як тод сприймав чужу малофю . Та тим б льше на сво му т л . Але справа була зроблена: хлопця 19 рок в видрали в дупу! спустили сперму йому в очко! В струнку, пружну, н жну попочку з н жною д рочкою, засунувши в не перший раз член! У перший момент було огидно в д того що щось липке, спочатку тепле, ст кало по стегнах, а пот м застигло так: (а так як не готувався до цього, то витерти було н чим:) Тод було прикро, не за те що ви бав, а що не попередивши, спустив в мене. Так як тод сперма сприймалася як щось мерзенне, тим б льше на соб . Пот м згадував про це вже з та мним насолодою, нав ть бажанням, щоб це повторилося: я поб г швидко з цього м сця, скор ше в д нього, а липка р дина на стегнах весь час нагадувала, що мене т льки що ви бли в жопу. Слава Укра н !\n",
      "2. ДА КАКОГО ЕБАНОГО ХУЯ МНЕ ТЕПЕРЬ ЮТУБ РЕКОМЕНДУЕТ ЕБУЧЕГО ШЕВЦОВА НАХУЙ СУКА БЛЯДЬ? Я КЛЯНУСЬ ЖОПОЙ БЛЯДЬ, Я НА ТРУБУ ЗАХОЖУ ТОЛЬКО САУНДТРЕКИ ИЗ ФИЛЬМ И ИГОР ПОСЛУШАТЬ. ЧТОБ ВАШЕГО АЙТИПЕДИЮ РАСКАЛЕННЫМИ КОЧЕРГАМИ В ЖОПУ ШПИОН ОТ ГУГОЛА ЕБАЛ, БЛЯДЬ. ЭТО ЖЕ ВСЕ ИЗ-ЗА ТОГО, ЧТО Я НА ХРЮ ЗАШЕЛ ПО НАВОДКЕ АБУ, ДА? МЕЙЛААААААААААААААААААААААААААААААЧ\n",
      "\n",
      "3. Как известно, у Укр ины (т.е. окр ины), слепленной по пьяни на коленке во 2-м десятилетии XX в., нет истории до XX столетия. Все земли, которые сейчас занимает Укр ина, это русские, румынские, польские и венгерские земли. Напоминаем, что укр инство это сугубо левацкая, антиконсервативная местечково-хуторская идеология, направленная, как и прочие левацкие идеи, на разделение больших наций и поддержание диктата интернацистов. Сторонники бандеровцев (леваков, выступавших за бесклассовое общество и борьбу с капитализмом) и карлика-душителя котов Степана Бандеры, который, как известно, боролся с расизмом, поддерживал Идель-Урал и называл побратимами исламских борцов за свободу из Азербайджана, не пользуются симпатиями у правых европейцев. И это правильно. Попытки заявить о некой отдельной нации неких украинцев это манипуляции, созданные с целью оторвать от русских часть их этнических земель и ослабить в будущем Россию. Только так, чудовищной ложью и тотальной пропагандой, фейковая нация укр инцев , слепленная советскими кукловодами из русских Юга и Киевщины, галичан, поляков, советских румын, славянизированных гуцулов, закарпатских венгров, евреев, татар и многонациональных советских новиопов (а ля Бабченко), может обрести жизнь на русских этнических землях. Разумеется, нет никакого народа укр инцев , как бы одно соседнее failed state ни пыталось их вывести из русских путём обмана, коверканья истории и откровенной фальсификации. Нынешний эксперимент по созданию некой украинской нации можно сравнить разве что с советским экспериментом по созданию нации советской на основании таких же мифов, фейков и откровенного бреда. И маниакальное желание снести все памятники выродку Ленину (Бланку) вас не должно обнадёживать. На смену ему устанавливают памятники такого же левацкого дегенерата-кошкодава Бандеры, чьи руки по локоть в славянской (прежде всего, польской) крови. Заместо совковой лжи про Великую революцию Октября пришла точно такая же наглая ложь про Великую революц ю Г дност абстрагируйтесь от фигуры блогера и посмотрите видео Чем в итоге завершился советский эксперимент, мы все знаем. Ждём закономерного итога эксперимента эльфийского (простите, укр инского ). Разумеется, зомбированные люди будут цепляться до последнего за свои мифы про отельный народ и чужих московитов , но всё это наваждение рано или поздно сгинет, как сгинул Совок со своей мощной идеологией, мифами и фейками.\n",
      "4. Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\n",
      "\n",
      "5. хорошо тут все ясно а теперь поясни мне за то что он кладет на репорты за шитпостинг и нерелейтед. айробусом дедом, лизон создали тред, карине создали тред, оляше создали тред, так какого хуя этим ебланам треды отдельные не создать? какого хуя ты тут чертила свой рот открываешь, тварь малолетняя, вот из за таких как ты уебанов все и скатилось в гавно, потому что черта этого никто не репортит, а такие говноеды как ты все сливают в парашу\n",
      "\n",
      "6. Ты забыл как сам тут в 2013 срал? Я понимаю что хохлы после прочтения неприятных постов впадают в депрессию, но это все последствия их квасного национализма. И прекрати называть хохлов людьми, хохол не может быть человеком.\n",
      "\n",
      "7. Зальете шебм? Вот этот кун. Говорит, будто заявление забрали. Алсо как-то и не скажешь, что он станет совать хуй в очъко тян, а потом обмазывать ей губы, лол. Фу нахуй. Алсо, мне думается, что пиздит\n",
      "\n",
      "8. ИДИ НАХУЙ, СУКА, ЗАВАЛИ ЕБАЛО, БЛЯТЬ\n",
      "\n",
      "9. Иди нахуй,бля. Я искал эту хуйню так как дрочу в гостиницах, когда в командировки летаю. Сплю бухой без трусов. Мне нужная эта ебень.\n",
      "\n",
      "10. Да в смысле блять добротных? Брат - это очередной АУЕ-высер по типу бумера или бригады. Кроме передачи эпохи 90-х в нем больше ничего нет. Зато современное быдло на главного героя равняется и со своей правдой идет грабить бить убивать. Заклеймить и запретить нахуй к показу.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.84       959\n",
      "         1.0       0.68      0.65      0.67       483\n",
      "\n",
      "    accuracy                           0.78      1442\n",
      "   macro avg       0.75      0.75      0.75      1442\n",
      "weighted avg       0.78      0.78      0.78      1442\n",
      "\n",
      "1. ИДИ НАХУЙ, СУКА, ЗАВАЛИ ЕБАЛО, БЛЯТЬ\n",
      "\n",
      "2. ой а кто-то ждал от хохлов не хуйни....\n",
      "\n",
      "3. да пошел ты нахуй сашенька ебаный\n",
      "\n",
      "4. Канада пристанище куколдов хохлов, не удивительно что они рашку хейтят.\n",
      "\n",
      "5. Сука, как же я обосрался с этого шерлока, до слёз блять\n",
      "\n",
      "6. Нахуй ты высрался тогда тут? МНЕ НИНРАВИЦА ЯСКОЗАЛ ХРЯ\n",
      "\n",
      "7. Да ты пидор какляцкий. В жопу тебя ебут.\n",
      "\n",
      "8. Забыл опиздюливание хохлов\n",
      "\n",
      "9. Мерзкий, тупой но сука смешной\n",
      "\n",
      "10. Там хохлов, больше чем тебя.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.96      0.88       959\n",
      "         1.0       0.88      0.55      0.68       483\n",
      "\n",
      "    accuracy                           0.83      1442\n",
      "   macro avg       0.85      0.76      0.78      1442\n",
      "weighted avg       0.83      0.83      0.81      1442\n",
      "\n",
      "1. ИДИ НАХУЙ, СУКА, ЗАВАЛИ ЕБАЛО, БЛЯТЬ\n",
      "\n",
      "2. ДА КАКОГО ЕБАНОГО ХУЯ МНЕ ТЕПЕРЬ ЮТУБ РЕКОМЕНДУЕТ ЕБУЧЕГО ШЕВЦОВА НАХУЙ СУКА БЛЯДЬ? Я КЛЯНУСЬ ЖОПОЙ БЛЯДЬ, Я НА ТРУБУ ЗАХОЖУ ТОЛЬКО САУНДТРЕКИ ИЗ ФИЛЬМ И ИГОР ПОСЛУШАТЬ. ЧТОБ ВАШЕГО АЙТИПЕДИЮ РАСКАЛЕННЫМИ КОЧЕРГАМИ В ЖОПУ ШПИОН ОТ ГУГОЛА ЕБАЛ, БЛЯДЬ. ЭТО ЖЕ ВСЕ ИЗ-ЗА ТОГО, ЧТО Я НА ХРЮ ЗАШЕЛ ПО НАВОДКЕ АБУ, ДА? МЕЙЛААААААААААААААААААААААААААААААЧ\n",
      "\n",
      "3. Сука, как же я обосрался с этого шерлока, до слёз блять\n",
      "\n",
      "4. Ты забыл как сам тут в 2013 срал? Я понимаю что хохлы после прочтения неприятных постов впадают в депрессию, но это все последствия их квасного национализма. И прекрати называть хохлов людьми, хохол не может быть человеком.\n",
      "\n",
      "5. Мерзкий, тупой но сука смешной\n",
      "\n",
      "6. Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\n",
      "\n",
      "7. И ещё один, какая же русня дегенераты пиздец просто.\n",
      "\n",
      "8. НУ ЧО СУКА ПРОСИЛ СКЕТЧИ? ПРОСИЛ? ПОЛУЧАЙ БЛЯДЬ!!\n",
      "\n",
      "9. где расчехляет кулаки, ты врешь 4пик Заебала пиздеть, русня\n",
      "\n",
      "10. Там хохлов, больше чем тебя.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_russian = list(stopwords.words('russian'))\n",
    "#понимаю, что в set читалось бы быстрее, но у меня почему-то не получилось придумать, как в векторайзер вставить сет, он ругался сильно\n",
    "\n",
    "\n",
    "data = pd.read_csv('labeled.csv')\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "\n",
    "#первая машинка\n",
    "vectorizer1 = CountVectorizer(min_df=10, max_df=0.3, stop_words=stop_words_russian, lowercase=True, encoding='UTF-8')\n",
    "X1 = vectorizer1.fit_transform(train.comment)\n",
    "X_test1 = vectorizer1.transform(test.comment)\n",
    "\n",
    "\n",
    "y1 = train.toxic.values\n",
    "y_test1 = test.toxic.values\n",
    "\n",
    "\n",
    "clf1 = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf1.fit(X1, y1)\n",
    "\n",
    "\n",
    "preds1 = clf1.predict(X_test1)\n",
    "\n",
    "print(classification_report(y_test1, preds1, zero_division=0))\n",
    "\n",
    "probs1 = clf1.predict_proba(X_test1)[:, 1]\n",
    "test['toxic_prob1'] = probs1\n",
    "\n",
    "top_10_sentences1 = test.sort_values(by='toxic_prob1', ascending=False).head(10)['comment']\n",
    "\n",
    "for i, sentence in enumerate(top_10_sentences1, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "#вторая машинка\n",
    "vectorizer2 = TfidfVectorizer(min_df=10, max_df=0.3, stop_words=stop_words_russian, lowercase=True, encoding='UTF-8')\n",
    "X2 = vectorizer2.fit_transform(train.comment)\n",
    "X_test2 = vectorizer2.transform(test.comment)\n",
    "\n",
    "\n",
    "y2 = train.toxic.values\n",
    "y_test2 = test.toxic.values\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "clf2.fit(X2, y2)\n",
    "\n",
    "\n",
    "preds2 = clf2.predict(X_test2)\n",
    "\n",
    "\n",
    "print(classification_report(y_test2, preds2, zero_division=0))\n",
    "\n",
    "probs2 = clf2.predict_proba(X_test2)[:, 1]\n",
    "test['toxic_prob2'] = probs2\n",
    "\n",
    "top_10_sentences2 = test.sort_values(by='toxic_prob2', ascending=False).head(10)['comment']\n",
    "\n",
    "for i, sentence in enumerate(top_10_sentences2, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "#третья машинка\n",
    "vectorizer3 = TfidfVectorizer(min_df=10, max_df=0.3, stop_words=stop_words_russian, lowercase=True, encoding='UTF-8')\n",
    "X3 = vectorizer3.fit_transform(train.comment)\n",
    "X_test3 = vectorizer3.transform(test.comment)\n",
    "\n",
    "\n",
    "y3 = train.toxic.values\n",
    "y_test3 = test.toxic.values\n",
    "\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(X3, y3)\n",
    "\n",
    "\n",
    "preds3 = clf3.predict(X_test3)\n",
    "\n",
    "\n",
    "print(classification_report(y_test3, preds3, zero_division=0))\n",
    "\n",
    "probs3 = clf3.predict_proba(X_test3)[:, 1]\n",
    "test['toxic_prob3'] = probs3\n",
    "\n",
    "top_10_sentences3 = test.sort_values(by='toxic_prob3', ascending=False).head(10)['comment']\n",
    "\n",
    "for i, sentence in enumerate(top_10_sentences3, 1):\n",
    "    print(f\"{i}. {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9008513-16f0-4e35-8a0a-fb581538c350",
   "metadata": {},
   "source": [
    "3. Кажется, у меня что-то сломалось с топом в NB, мне кажется, такие слова должны были вообще на этапе стоп-слов улететь, но у меня уже нет сил это в 10 раз переписывать, там одно и то же получается("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ef77357-9016-446b-84bc-871d9694eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sofya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words_russian = list(stopwords.words('russian'))\n",
    "\n",
    "data = pd.read_csv('labeled.csv')\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=0.3, stop_words=stop_words_russian)\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment)\n",
    "\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13e52d-fd5f-4f8d-bdce-c6a114d39449",
   "metadata": {},
   "source": [
    "Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0378a7-3a9e-4452-9ba1-b10cdaf62c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.77      0.83       968\n",
      "         1.0       0.64      0.83      0.73       474\n",
      "\n",
      "    accuracy                           0.79      1442\n",
      "   macro avg       0.77      0.80      0.78      1442\n",
      "weighted avg       0.82      0.79      0.80      1442\n",
      "\n",
      "     Feature1  Coefficient1\n",
      "3092    хохлы      1.496368\n",
      "3091   хохлов      1.458885\n",
      "2833     тебе      1.252565\n",
      "539     дебил      1.207219\n",
      "1418    нахуй      1.074247\n",
      "1811   пиздец      1.051941\n",
      "2767     сука      1.031327\n",
      "159     блядь      0.975234\n",
      "2384  русских      0.964951\n",
      "2923    тупые      0.952257\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf1.fit(X, y)\n",
    "\n",
    "\n",
    "preds1 = clf1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds1, zero_division=0))\n",
    "\n",
    "feature_names1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "coefficients1 = clf1.coef_[0]\n",
    "\n",
    "coefficients_df1 = pd.DataFrame({'Feature1': feature_names1, 'Coefficient1': coefficients1})\n",
    "\n",
    "top_words1 = coefficients_df1.sort_values(by='Coefficient1', ascending=False).head(10)\n",
    "\n",
    "print(top_words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8506ab-8b14-4d55-9c40-110ac53ec02a",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24d29e7-6388-4293-a4d6-15c487c294a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.84      0.84       968\n",
      "         1.0       0.66      0.66      0.66       474\n",
      "\n",
      "    accuracy                           0.78      1442\n",
      "   macro avg       0.75      0.75      0.75      1442\n",
      "weighted avg       0.78      0.78      0.78      1442\n",
      "\n",
      "      Feature2  Importance2\n",
      "1742     очень     0.009075\n",
      "2833      тебе     0.008370\n",
      "3092     хохлы     0.008005\n",
      "3091    хохлов     0.007603\n",
      "3219       это     0.007478\n",
      "1418     нахуй     0.006637\n",
      "1395  например     0.005924\n",
      "863       знаю     0.005526\n",
      "2651   спасибо     0.005054\n",
      "1139       лет     0.005030\n"
     ]
    }
   ],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "clf2.fit(X, y)\n",
    "\n",
    "\n",
    "preds2 = clf2.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, preds2, zero_division=0))\n",
    "\n",
    "feature_names2 = vectorizer.get_feature_names_out()\n",
    "\n",
    "importances2 = clf2.feature_importances_\n",
    "\n",
    "importances_df2 = pd.DataFrame({'Feature2': feature_names2, 'Importance2': importances2})\n",
    "\n",
    "top_words2 = importances_df2.sort_values(by='Importance2', ascending=False).head(10)\n",
    "\n",
    "print(top_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e113e0-4f3b-453a-9e43-2ab1b28ebb8b",
   "metadata": {},
   "source": [
    "Наивная байесовская"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e628f960-5e11-4065-b2b1-a18765d12767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.93      0.88       968\n",
      "         1.0       0.82      0.63      0.71       474\n",
      "\n",
      "    accuracy                           0.83      1442\n",
      "   macro avg       0.83      0.78      0.79      1442\n",
      "weighted avg       0.83      0.83      0.82      1442\n",
      "\n",
      "     Feature3  Log Probability3\n",
      "3219      это         -3.461941\n",
      "2185   просто         -4.508633\n",
      "2833     тебе         -4.932117\n",
      "377       всё         -5.275888\n",
      "721       ещё         -5.369917\n",
      "337    вообще         -5.376984\n",
      "2039   почему         -5.398491\n",
      "1418    нахуй         -5.538252\n",
      "3092    хохлы         -5.662305\n",
      "1086  которые         -5.720574\n"
     ]
    }
   ],
   "source": [
    "clf3 = MultinomialNB()\n",
    "clf3.fit(X, y)\n",
    "\n",
    "\n",
    "preds3 = clf3.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds3, zero_division=0))\n",
    "\n",
    "feature_names3 = vectorizer.get_feature_names_out()\n",
    "\n",
    "log_probabilities3 = clf3.feature_log_prob_\n",
    "\n",
    "log_probabilities_df3 = pd.DataFrame({'Feature3': feature_names3, 'Log Probability3': log_probabilities3[1]})\n",
    "\n",
    "top_words3 = log_probabilities_df3.sort_values(by='Log Probability3', ascending=False).head(10)\n",
    "\n",
    "print(top_words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256361d-6d77-4518-b8a8-0f7a3ae57d72",
   "metadata": {},
   "source": [
    "Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71efdf40-af58-4bf5-a20a-64dc9f48adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.77      0.83       968\n",
      "         1.0       0.64      0.83      0.73       474\n",
      "\n",
      "    accuracy                           0.79      1442\n",
      "   macro avg       0.77      0.80      0.78      1442\n",
      "weighted avg       0.82      0.79      0.80      1442\n",
      "\n",
      "      Feature4  Importance4\n",
      "2833      тебе     0.012936\n",
      "1742     очень     0.012603\n",
      "3092     хохлы     0.011437\n",
      "1418     нахуй     0.010158\n",
      "3091    хохлов     0.009492\n",
      "1139       лет     0.009368\n",
      "3219       это     0.008314\n",
      "863       знаю     0.008221\n",
      "1395  например     0.007161\n",
      "159      блядь     0.006100\n"
     ]
    }
   ],
   "source": [
    "clf4 = DecisionTreeClassifier(class_weight='balanced')  \n",
    "clf4.fit(X, y)\n",
    "\n",
    "preds4 = clf4.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds1, zero_division=0))\n",
    "\n",
    "\n",
    "feature_names4 = vectorizer.get_feature_names_out()\n",
    "\n",
    "importances_tree4 = clf4.feature_importances_\n",
    "\n",
    "importances_tree_df4 = pd.DataFrame({'Feature4': feature_names4, 'Importance4': importances_tree4})\n",
    "\n",
    "top_words4 = importances_tree_df4.sort_values(by='Importance4', ascending=False).head(10)\n",
    "\n",
    "print(top_words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04acfca-d217-4bcb-a784-16627e857488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
